{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"610287d4c32b4accb0d3cc5b2d6ec755":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ef8d649f4a6a4541aaf49f757d4a7d14","IPY_MODEL_70c9de6eb108484f885075dc5e9b2bc3","IPY_MODEL_6a1d786c0f2c419392e6f9e360feb256"],"layout":"IPY_MODEL_5b5f5aa4cdff4e9683281f79150ba441"}},"ef8d649f4a6a4541aaf49f757d4a7d14":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_54ec87c3e1ae423fb78117823221bc29","placeholder":"​","style":"IPY_MODEL_944b3922b99f461d9b6924d0e4f0aecc","value":"Loading checkpoint shards:  25%"}},"70c9de6eb108484f885075dc5e9b2bc3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_f09e11c473bc425d807ae8bce21d267e","max":8,"min":0,"orientation":"horizontal","style":"IPY_MODEL_744703202e914102999ff9faf197d04e","value":2}},"6a1d786c0f2c419392e6f9e360feb256":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_270d71856d5e4054a4759c9fea194844","placeholder":"​","style":"IPY_MODEL_ec329caef6d0412cb363f588c8862724","value":" 2/8 [00:17&lt;00:53,  8.89s/it]"}},"5b5f5aa4cdff4e9683281f79150ba441":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54ec87c3e1ae423fb78117823221bc29":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"944b3922b99f461d9b6924d0e4f0aecc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f09e11c473bc425d807ae8bce21d267e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"744703202e914102999ff9faf197d04e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"270d71856d5e4054a4759c9fea194844":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec329caef6d0412cb363f588c8862724":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11742618,"sourceType":"datasetVersion","datasetId":7371429},{"sourceId":383460,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":316505,"modelId":337018}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q google-generativeai transformers tensorflow keras PyMuPDF huggingface-hub evaluate accelerate gradio bitsandbytes","metadata":{"id":"XFy8gdNYgi6v","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:47:46.107120Z","iopub.execute_input":"2025-05-09T10:47:46.107672Z","iopub.status.idle":"2025-05-09T10:49:14.759823Z","shell.execute_reply.started":"2025-05-09T10:47:46.107621Z","shell.execute_reply":"2025-05-09T10:49:14.759116Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m85.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport gc\nimport csv\nimport fitz  # PyMuPDF\nimport torch\nimport pandas as pd\nimport gradio as gr\nimport google.generativeai as genai\nfrom huggingface_hub import HfFolder\nfrom kaggle_secrets import UserSecretsClient\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n# Environment setup\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:57:48.566373Z","iopub.execute_input":"2025-05-09T10:57:48.566687Z","iopub.status.idle":"2025-05-09T10:57:49.245154Z","shell.execute_reply.started":"2025-05-09T10:57:48.566660Z","shell.execute_reply":"2025-05-09T10:57:49.244568Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"secrets = UserSecretsClient()\ngenai.configure(api_key=secrets.get_secret(\"API_KEY\"))\nHfFolder.save_token(secrets.get_secret(\"HF_TOKEN\"))","metadata":{"id":"O7wz3Qu1NXa5","scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:57:49.246977Z","iopub.execute_input":"2025-05-09T10:57:49.247181Z","iopub.status.idle":"2025-05-09T10:57:49.486866Z","shell.execute_reply.started":"2025-05-09T10:57:49.247166Z","shell.execute_reply":"2025-05-09T10:57:49.486203Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport csv\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nimport tensorflow as tf\ndef load_models():\n    model1 = None\n    if os.path.exists(\"/kaggle/input/diabetes/tensorflow2/default/1/mlp_model_weights.weights.h5\"):\n        with tf.device('/CPU:0'):   # ← Force-load on CPU\n            model1 = Sequential([\n                Dense(256, activation='relu', input_shape=(5,)),  # Adjusted input shape to match 5 features\n                Dropout(0.3),\n                Dense(128, activation='relu'),\n                Dropout(0.3),\n                Dense(64, activation='relu'),\n                Dropout(0.3),\n                Dense(1, activation='sigmoid')\n            ])\n            model1.load_weights('/kaggle/input/diabetes/tensorflow2/default/1/mlp_model_weights.weights.h5')\n    return model1\n\nmodel1 = load_models()\n\n# -------------------------\n#  PDF PARSING FUNCTIONS\n# -------------------------\n\nimport fitz  # PyMuPDF\nimport google.generativeai as genai\n\ndef extract_text_from_pdf(pdf_path):\n    \"\"\"Extract text from a PDF file while preserving horizontal structure.\"\"\"\n    doc = fitz.open(pdf_path)\n    extracted_text = []\n    for page in doc:\n        blocks = page.get_text(\"blocks\")  # Extract text in blocks\n        sorted_blocks = sorted(blocks, key=lambda b: (b[1], b[0]))  # Sort by vertical then horizontal\n        page_text = \"\\n\".join(block[4] for block in sorted_blocks)\n        extracted_text.append(page_text)\n    return \"\\n\".join(extracted_text)\n\ndef structure_text_with_gemini(text):\n    \"\"\"\n    Send extracted text to Gemini for tabular structuring and CSV formatting,\n    including detailed status analysis.\n    \"\"\"\n    model_1 = genai.GenerativeModel(\"gemini-2.0-flash\")\n\n    prompt = (\n        \"Extract structured tabular data from the following medical report and format it as CSV with proper headers.\\n\"\n        \"Ensure the column headers exactly match the following list:\\n\"\n        \"age, bmi, HbA1c_level, blood_glucose_level, diabetes\\n\\n\"\n        \"Return all values numerically in CSV format with one row.\\n\\n\"\n        \"After the CSV, also detect whether the patient shows signs of **anemia**, based on any of the following:\\n\"\n        \"- Low Hemoglobin (e.g., <13.0 for men, <12.0 for women)\\n\"\n        \"- Low Hematocrit\\n\"\n        \"- Descriptions like 'anemic', 'iron deficiency', etc.\\n\"\n        \"Append a final line like this: anemia_flag: 1 (if anemia detected), otherwise anemia_flag: 0.\\n\\n\"\n        f\"{text}\"\n    )\n    response = model_1.generate_content(prompt)\n    return response.text.strip() if response.text else \"\"\n\ndef save_csv(csv_text, output_csv):\n    \"\"\"Save the structured CSV text into a file with consistent columns.\"\"\"\n    rows = [line.split(\",\") for line in csv_text.split(\"\\n\") if line.strip()]\n    max_cols = max(len(row) for row in rows) if rows else 0\n    normalized_rows = [row + [\"\"] * (max_cols - len(row)) for row in rows]\n    with open(output_csv, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n        writer = csv.writer(file)\n        writer.writerows(normalized_rows)\n\ndef get_model():\n    return secrets.get_secret(\"Models\")\n\ndef pdf_to_csv(pdf_path, output_csv):\n    \"\"\"Complete pipeline: Extract, structure, and save as CSV.\"\"\"\n    text = extract_text_from_pdf(pdf_path)\n    csv_text = structure_text_with_gemini(text)\n    save_csv(csv_text, output_csv)\n    return output_csv\n\n# -------------------------\n#  MAPPING FUNCTIONS\n# -------------------------\n\ndef map_data_for_model1(csv_file):\n    \"\"\"\n    Read the CSV produced from the PDF and map its data to the input features for model 1.\n    If some values are missing, use safe default values.\n    \"\"\"\n    defaults = {\n        \"age\": 43,\n        \"bmi\": 27.32,\n        \"HbA1c_level\": 5.8,\n        \"blood_glucose_level\": 140.0,\n        \"diabetes\": 0\n    }\n    try:\n        df = pd.read_csv(csv_file)\n        model1_input = {\n            key: df[key].iloc[0] if key in df.columns and not pd.isna(df[key].iloc[0]) else default\n            for key, default in defaults.items()\n        }\n    except Exception as e:\n        print(\"Error reading CSV: \" + str(e))\n        model1_input = defaults\n    return model1_input\n","metadata":{"id":"3O9x_Iufhplh","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:57:51.720403Z","iopub.execute_input":"2025-05-09T10:57:51.720731Z","iopub.status.idle":"2025-05-09T10:57:51.786396Z","shell.execute_reply.started":"2025-05-09T10:57:51.720711Z","shell.execute_reply":"2025-05-09T10:57:51.785584Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    bnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_use_double_quant=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16\n)\n)\n\nmodel_id = \"epfl-llm/meditron-7b\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    device_map=\"auto\",\n    quantization_config=bnb_config\n)\n\ntokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["610287d4c32b4accb0d3cc5b2d6ec755","ef8d649f4a6a4541aaf49f757d4a7d14","70c9de6eb108484f885075dc5e9b2bc3","6a1d786c0f2c419392e6f9e360feb256","5b5f5aa4cdff4e9683281f79150ba441","54ec87c3e1ae423fb78117823221bc29","944b3922b99f461d9b6924d0e4f0aecc","f09e11c473bc425d807ae8bce21d267e","744703202e914102999ff9faf197d04e","270d71856d5e4054a4759c9fea194844","ec329caef6d0412cb363f588c8862724"]},"id":"Dlf4Kk0rOcgs","outputId":"0a49e110-ad89-4f80-9a9c-3c4d2a54aeda","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T10:51:41.883795Z","iopub.execute_input":"2025-05-09T10:51:41.885108Z","iopub.status.idle":"2025-05-09T10:52:05.979040Z","shell.execute_reply.started":"2025-05-09T10:51:41.885078Z","shell.execute_reply":"2025-05-09T10:52:05.978438Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/8 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7745b808c6e54893962ac7a4aed36f39"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def generate_meditron_prompt(structured_csv, diabetes_result):\n    return f\"\"\"\nYou are a clinical report generator using blood biomarkers. Your task is to generate a structured, medically accurate patient report based ONLY on the following lab data and diabetes risk assessment.\n\nStructured Lab Data:\n{structured_csv}\n\nDiabetes prediction (from independent model): {diabetes_result}\n\nWrite a detailed report including:\n1. Potential health vulnerabilities (CBC, Lipid Profile, etc.)\n2. Suggested lifestyle changes and dietary guidance.\n3. Natural/home remedies (if relevant).\n4. Recommended medical specialists to consult.\n\n- Format in professional bullet points.\n- Limit to **approximately 500 tokens**.\n- Be concise, medical, and actionable.\n- Don't use markdown, stick to simple text.\n\"\"\"\n\n\nimport tensorflow as tf\n\ndef predict_diabetes_cpu(model, input_df):\n    with tf.device('/CPU:0'):\n        return model.predict(input_df)\n\ndef generate_report(pdf_file):\n    pdf_path = pdf_file.name\n    \n    extracted_text = extract_text_from_pdf(pdf_path)\n    structured_text = structure_text_with_gemini(extracted_text)\n    \n    temp_csv_path = \"structured_report.csv\"\n    save_csv(structured_text, temp_csv_path)\n    model1_input = map_data_for_model1(temp_csv_path)\n\n    # Strict feature order\n    FEATURE_ORDER = [\"age\", \"bmi\", \"HbA1c_level\", \"blood_glucose_level\", \"diabetes\"]\n    input_df = pd.DataFrame([model1_input])[FEATURE_ORDER]\n    \n    # Predict diabetes safely\n    with tf.device('/CPU:0'):\n        diabetes_prob = model1.predict(input_df)[0][0]\n\n    diabetes_result = \"Positive (high risk)\" if diabetes_prob >= 0.5 else \"Negative (low risk)\"\n    \n    prompt = generate_meditron_prompt(structured_text, diabetes_result)\n\n    model_1 = genai.GenerativeModel(\"gemini-2.0-flash\")\n    response = model_1.generate_content(prompt)\n    \n    # Clean up Gemini output\n    generated_text = response.text.strip()\n    if \"<END>\" in generated_text:\n        generated_text = generated_text.split(\"<END>\")[0].strip()\n    \n    final_report = generated_text + f\"\\n\\n🩺 **Diabetes Prediction:** {diabetes_result}\"\n\n    with open(\"generated_report.txt\", \"w\") as f:\n        f.write(final_report)\n\n    return final_report, \"generated_report.txt\"\n","metadata":{"id":"NgjtqP0XQF2R","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:03:26.717475Z","iopub.execute_input":"2025-05-09T11:03:26.718162Z","iopub.status.idle":"2025-05-09T11:03:26.728254Z","shell.execute_reply.started":"2025-05-09T11:03:26.718140Z","shell.execute_reply":"2025-05-09T11:03:26.727599Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"interface = gr.Interface(\n    fn=generate_report,\n    inputs=gr.File(label=\"Upload Medical Report PDF\", file_types=[\".pdf\"]),\n    outputs=[\n        gr.Textbox(label=\"Generated Medical Report\"),\n        gr.File(label=\"Download Report\")\n    ],\n    title=\"🧠 Meditron-7B Medical Report Generator\",\n    description=\"Upload a CBC report PDF. The app extracts key features, analyzes them with Meditron-7B, and generates a ~500-token structured medical report.\"\n)\n\ninterface.launch(share=True)","metadata":{"id":"c65MLKmQobUx","trusted":true,"execution":{"iopub.status.busy":"2025-05-09T11:03:27.024522Z","iopub.execute_input":"2025-05-09T11:03:27.024714Z","iopub.status.idle":"2025-05-09T11:03:27.594514Z","shell.execute_reply.started":"2025-05-09T11:03:27.024700Z","shell.execute_reply":"2025-05-09T11:03:27.593742Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7867\n* Running on public URL: https://b9db247731905b3e8c.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://b9db247731905b3e8c.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":""},"metadata":{}},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}