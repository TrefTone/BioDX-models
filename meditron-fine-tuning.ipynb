{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31011,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "notebooke3668f1665",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# --- Install required packages ---\n",
        "!pip install -q torch transformers accelerate peft datasets huggingface_hub bitsandbytes"
      ],
      "metadata": {
        "id": "installation",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:29.757779Z",
          "iopub.execute_input": "2025-05-01T10:01:29.758295Z",
          "iopub.status.idle": "2025-05-01T10:01:36.825471Z",
          "shell.execute_reply.started": "2025-05-01T10:01:29.758268Z",
          "shell.execute_reply": "2025-05-01T10:01:36.82468Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Hugging Face authentication ---\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "from huggingface_hub import HfFolder\n",
        "user_secrets = UserSecretsClient()\n",
        "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
        "HfFolder.save_token(hf_token)"
      ],
      "metadata": {
        "id": "hf_token",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:36.827122Z",
          "iopub.execute_input": "2025-05-01T10:01:36.827479Z",
          "iopub.status.idle": "2025-05-01T10:01:36.895288Z",
          "shell.execute_reply.started": "2025-05-01T10:01:36.827455Z",
          "shell.execute_reply": "2025-05-01T10:01:36.894589Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Imports ---\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,\n",
        "    Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        ")\n",
        "from datasets import load_dataset, Dataset\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training"
      ],
      "metadata": {
        "id": "import",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:36.896072Z",
          "iopub.execute_input": "2025-05-01T10:01:36.896416Z",
          "iopub.status.idle": "2025-05-01T10:01:38.908645Z",
          "shell.execute_reply.started": "2025-05-01T10:01:36.896391Z",
          "shell.execute_reply": "2025-05-01T10:01:38.908092Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Quantization Config ---\n",
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "# --- Load Model ---\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"epfl-llm/meditron-7b\",\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=quant_config,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# --- Tokenizer ---\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"epfl-llm/meditron-7b\", trust_remote_code=True)\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "quant_model",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:38.910137Z",
          "iopub.execute_input": "2025-05-01T10:01:38.910617Z",
          "iopub.status.idle": "2025-05-01T10:01:39.10791Z",
          "shell.execute_reply.started": "2025-05-01T10:01:38.910599Z",
          "shell.execute_reply": "2025-05-01T10:01:39.106987Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load and convert datasets ---\n",
        "ds1 = load_dataset(\"lavita/ChatDoctor-HealthCareMagic-100k\", split=\"train[:25000]\")\n",
        "ds2 = load_dataset(\"mlabonne/FineTome-100k\", split=\"train[:25000]\")\n",
        "\n",
        "def convert_chatdoctor(example):\n",
        "    return {\n",
        "        \"text\": f\"<s>### Instruction:\\n{example['instruction']}\\n{example['input']}\\n\\n### Response:\\n{example['output']}</s>\"\n",
        "    }\n",
        "\n",
        "def convert_finetome(example):\n",
        "    conv = example[\"conversations\"]\n",
        "    user_msg = next((m[\"value\"] for m in conv if m[\"from\"] == \"human\"), \"\")\n",
        "    assistant_msg = next((m[\"value\"] for m in conv if m[\"from\"] == \"gpt\"), \"\")\n",
        "    return {\n",
        "        \"text\": f\"<s>### Instruction:\\n{user_msg}\\n\\n### Response:\\n{assistant_msg}</s>\"\n",
        "    }\n",
        "\n",
        "ds1 = ds1.map(convert_chatdoctor, remove_columns=ds1.column_names)\n",
        "ds2 = ds2.map(convert_finetome, remove_columns=ds2.column_names)\n",
        "from datasets import concatenate_datasets\n",
        "merged_dataset = concatenate_datasets([ds1, ds2])"
      ],
      "metadata": {
        "id": "dataset",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:39.108259Z",
          "iopub.status.idle": "2025-05-01T10:01:39.108474Z",
          "shell.execute_reply.started": "2025-05-01T10:01:39.10837Z",
          "shell.execute_reply": "2025-05-01T10:01:39.10838Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Tokenize ---\n",
        "def tokenize(example):\n",
        "    output = tokenizer(\n",
        "        example[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "    output[\"labels\"] = output[\"input_ids\"].copy()\n",
        "    return output\n",
        "\n",
        "tokenized_dataset = merged_dataset.map(tokenize, remove_columns=[\"text\"], batched=True)"
      ],
      "metadata": {
        "id": "tokenize",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:39.109442Z",
          "iopub.status.idle": "2025-05-01T10:01:39.109685Z",
          "shell.execute_reply.started": "2025-05-01T10:01:39.109581Z",
          "shell.execute_reply": "2025-05-01T10:01:39.109592Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LoRA ---\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "lora",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:39.110681Z",
          "iopub.status.idle": "2025-05-01T10:01:39.110993Z",
          "shell.execute_reply.started": "2025-05-01T10:01:39.110837Z",
          "shell.execute_reply": "2025-05-01T10:01:39.110851Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Train ---\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./meditron-finetuned-50k\",\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    gradient_checkpointing=True,\n",
        "    warmup_steps=200,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    bf16=True,\n",
        "    save_total_limit=1,\n",
        "    save_strategy=\"epoch\",\n",
        "    report_to=\"none\",\n",
        "    optim=\"paged_adamw_32bit\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "train",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:39.111847Z",
          "iopub.status.idle": "2025-05-01T10:01:39.112156Z",
          "shell.execute_reply.started": "2025-05-01T10:01:39.111996Z",
          "shell.execute_reply": "2025-05-01T10:01:39.11201Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_model_id = \"epfl-llm/meditron-7b\"\n",
        "adapter_path = \"./meditron-finetuned-50k/checkpoint-1\"\n",
        "repo_id = \"samirangupta31/meditron-7b-finetuned-quantized"  # Change to your Hugging Face repo\n",
        "\n",
        "merged_model.push_to_hub(repo_id, use_auth_token=True)\n",
        "tokenizer.push_to_hub(repo_id, use_auth_token=True)"
      ],
      "metadata": {
        "id": "push",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-05-01T10:01:45.41187Z",
          "iopub.execute_input": "2025-05-01T10:01:45.412621Z",
          "iopub.status.idle": "2025-05-01T10:02:39.075937Z",
          "shell.execute_reply.started": "2025-05-01T10:01:45.412591Z",
          "shell.execute_reply": "2025-05-01T10:02:39.074216Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "RuPZSjnNH3-8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}
